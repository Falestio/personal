---
title: "Pengembangan Autoscaling Horizontal pada IDCloudHost Menggunakan Traefik Sebagai Load Balancer dan Grafana Alert"
description: "Pelajari cara mengimplementasikan autoscaling secara manual di IDCloudHost menggunakan IDCloudHost API, Traefik, dan Grafana Alert."
thumbnail: "/blog-img/idcloudhost-autoscaling/thumbnail-article-big.png"
category:
  - "Cloud Computing"
  - "IDCloudHost"
  - "Traefik"
  - "Grafana"
---

IDCloudHost menyediakan virtual machine (VM) dengan harga yang relatif terjangkau dibandingkan penyedia layanan cloud lainnya, seperti AWS atau DigitalOcean.

| Provider                                                       | Harga dalam USD | Harga dalam IDR |
| -------------------------------------------------------------- | --------------- | --------------- |
| AWS (On-Demand, T3 Small, 2 CPU, 2 GiB RAM, 60 GB EBS Storage) | $25.03          | Rp 391.939      |
| DigitalOcean Droplet (2 CPU, 2 GiB RAM, 60 GB Storage)         | $18.00          | Rp 281.826      |
| IDCloudHost (2 CPU, 2 GiB RAM, 60 GB Storage)                  | $7.19           | Rp 112.500      |

\*Data diambil pada 8 Oktober 2024

- Harga AWS diperoleh dari AWS Pricing Calculator.
- Harga DigitalOcean diperoleh dari halaman pricing Droplets.
- Harga IDCloudHost diperoleh dari halaman console saat membuat VM baru.
- Kurs: 1 Dollar = Rp 15.657 (8 Oktober 2024).

Meskipun harganya lebih terjangkau, IDCloudHost tidak memiliki fitur yang selengkap penyedia lain, seperti autoscaling.

Fitur autoscaling sangat penting untuk menangani peningkatan trafik secara otomatis, tanpa memerlukan intervensi manual. Saat ini, IDCloudHost menyediakan fitur self-upgrade dan self-downgrade pada layanannya. Meskipun memungkinkan scaling secara manual, fitur ini tidak seotomatis autoscaling pada layanan cloud besar lainnya.

Namun, IDCloudHost menyediakan API yang dapat digunakan untuk mengelola setiap sumber daya, termasuk membuat dan menghapus VM.

Artikel ini akan menjelaskan cara memanfaatkan API ini untuk keperluan autoscaling dengan bantuan Grafana _alert_ dan _load balancer_ dari Traefik.

## Gambaran Sistem yang Akan Dibuat

Gambar di bawah adalah gambaran umum dari sistem autoscaling yang akan dibuat:

:image-display{src="/blog-img/idcloudhost-autoscaling/general-archi.svg"}

- Kelompok _`service-clones`_  
  _Service clones_ adalah kumpulan _virtual machine_ yang akan _di-autoscale_. Kelompok virtual machine ini dibuat dari sebuah "service seed," yaitu virtual machine utama yang berfungsi sebagai template. Service seed berisi aplikasi dan konfigurasi yang diperlukan, dan setiap service clone yang dibuat merupakan salinan dari service seed.

- `Traefik API Gateway & Load Balancer`  
  Traefik adalah _HTTP reverse proxy_ yang sejenis dengan NGINX namun menawarkan konfigurasi yang lebih user-friendly. Pada artikel ini, Traefik akan digunakan sebagai _API gateway_ dan _load balancer_ yang bertugas sebagai lapisan pertama sebelum request.

  ::info-box{type="question" title="Kenapa tidak menggunakan load balancer dari IdCloudHost?"}
  Kendala utama dalam menggunakan load balancer bawaan IDCloudHost adalah keterbatasan pengendalian. Load balancer ini hanya mendukung metode session persistence yang tidak dapat diubah. Selain itu, biaya penggunaannya relatif tinggi yaitu Rp 150.000 per bulan dibanding membuat load balancer sendiri cukup dengan Rp 87.000 per bulan untuk konfigurasi 2 vCPU dan 2 GB RAM.
  ::

- `Grafana & Prometheus Monitoring Server`  
  Grafana dan Prometheus digunakan untuk monitoring dan pengiriman notifikasi. Prometheus berperan sebagai sumber data, sementara Grafana digunakan untuk visualisasi data dan penerapan fungsi alert yang dapat memicu scale up atau scale down.

- `IDCloudHost API`  
  Ini adalah API yang telah disediakan IDCloudHost. API ini memungkinkan pengguna berinteraksi dengan setiap resource yang ada di IDCloudHost, seperti membuat, menghapus, dan mengedit virtual machine.

- `IDCloudHost VPC Network`  
  VPC memungkinkan server berkomunikasi melalui jaringan privat, yang mengurangi area potensi serangan oleh hacker.

## Penjelasan Stack yang Dipilih

### Traefik

Traefik adalah _reverse proxy_ dan _load balancer_ _open-source_ yang dirancang untuk mengatur traffic HTTP dan TCP di lingkungan mikroservis. Traefik dapat dijadikan API gateway, load balancer, dan proxy untuk server VM, Docker, atau Kubernetes.

Traefik memiliki banyak turunan yang menangani _use case_ yang berbeda, namun yang akan kita gunakan adalah [Traefik Proxy](https://doc.traefik.io/traefik/).

Alasan memilih Traefik adalah kemampuannya menyimpan konfigurasi dalam file yang dinamis. Artinya, perubahan dapat dilakukan langsung pada file tersebut tanpa perlu me-restart virtual machine. Ini sangat penting dalam peristiwa scaling, di mana konfigurasi sering diperbarui untuk menambah atau mengurangi daftar service yang terhubung ke load balancer. Dengan tidak perlu me-restart VM, downtime dapat dihindari selama proses scaling.

Saat ini, Traefik hanya mendukung metode load balancing **round robin**.

### Grafana

Grafana adalah platform open-source yang berfungsi untuk menganalisis, memonitor, dan menvisualisasikan data dari berbagai macam sumber.

Pada artikel ini, Grafana akan digunakan untuk melakukan monitoring terhadap load dari server yang akan di-scaling.

Selain itu, Grafana juga memiliki fitur alert yang akan digunakan untuk menginisiasi peristiwa scale up atau scale down.

### Prometheus

[Prometheus](https://prometheus.io/docs/introduction/overview/) adalah tool open-source untuk monitoring dan alerting yang dirancang untuk mengumpulkan, menyimpan, dan menganalisis data layanan dan infrastruktur. Biasanya, Prometheus digunakan bersama Grafana untuk menvisualisasikan data.

Kita akan menggunakan Prometheus bersama Node Exporter.

Node Exporter adalah klien yang akan mengumpulkan dan mengirimkan data _system-level_ (penggunaan CPU, RAM, Disk) ke Prometheus.

## Alur Kerja Sistem

Gambar di bawah menunjukkan alur sistem bekerja:

:image-display{src="/blog-img/idcloudhost-autoscaling/cara-kerja-umum.svg"}

1. Pengguna mengirim permintaan ke endpoint yang diekspos oleh Traefik melalui HTTP atau HTTPS.
2. Traefik berfungsi sebagai load balancer dan meneruskan permintaan ke layanan sesuai giliran, menggunakan metode round robin.
3. Setiap service akan mengirimkan data ke server monitoring.
4. Ketika beban CPU melebihi atau kurang dari ambang batas yang ditentukan, server monitoring akan mengirim perintah ke API IDCloudHost untuk melakukan scale up atau scale down, menggunakan Grafana alert.
5. API IDCloudHost kemudian akan melakukan scaling. Jika scale up, service seed akan di-clone untuk membuat service baru. Jika melakukan scale down, VM terakhir yang di-clone akan dihapus.
6. Jika service clone berhasil dibuat (scale up), maka akan terhubung dengan Traefik dan server monitoring. Jika berhasil dihapus (scale down), koneksi antara Traefik dan server monitoring akan diputus.

## Penjelasan Lanjut Cara Kerja Scale Up

:image-display{src="/blog-img/idcloudhost-autoscaling/cara-kerja-detail-1.png"}

Saat sistem menerima banyak request, _system load_ akan meningkat. Jika ini terjadi, sistem akan gagal untuk memproses beberapa request, sehingga perlu dilakukan _scale up_ untuk menangani beban tambahan tersebut.

:image-display{src="/blog-img/idcloudhost-autoscaling/cara-kerja-detail-2.png"}

Tahapan untuk event scale-up dijelaskan pada gambar kedua:

1. Banyak _request_ yang datang menyebabkan layanan mengalami _load_ yang tinggi, yang pada akhirnya menghasilkan kegagalan memproses _request_.
2. Prometheus mengumpulkan data penggunaan sistem (CPU dan RAM usage).
3. Grafana mendeteksi tingkat penggunaan sistem di atas batas yang ditentukan. Sehingga event scale up terjadi dimulai dengan memanggil webhook scale up.
4. Handler dari webhook scale up akan berinteraksi dengan API IDCloudHost untuk melakukan cloning terhadap service seed.
5. Setelah VM service seed berhasil di-clone, webhook handler akan memperbarui daftar target di server Prometheus di server monitoring untuk menyertakan server yang baru dibuat.
6. Webhook handler kemudian akan mengirim permintaan ke target updater pada server Traefik untuk memperbarui daftar target load balancer, memasukkan server baru ke dalam daftar target.
7. Target updater pada server Traefik akan memperbarui daftar IP dan menambahkan alamat IP privat dari service clone yang baru.

::info-box{type="info" title="Tentang Daftar Target"}
Bentuk daftar target adalah URL yang menuju ke service clone/seed beserta dengan portnya. Daftar URL ini disimpan pada file konfigurasi Traefik dan Prometheus.
::

:image-display{src="/blog-img/idcloudhost-autoscaling/cara-kerja-detail-3.png"}

Gambar ketiga menunjukkan hasil dari operasi di atas:

1. Nomor satu menunjukkan service clone hasil dari langkah nomor 4 pada bagian sebelumnya. Semua aspek dari service seed akan di-clone, termasuk service itu sendiri dan konfigurasi Prometheus.
2. Koneksi telah terbentuk antara _service clone_ dengan server monitoring. Ini adalah hasil dari langkah ke-5 dari bagian sebelumnya.
3. Koneksi antara Traefik dan service baru terbentuk. Ini adalah hasil dari langkah ke-6 dari bagian sebelumnya.

## Penjelasan Lanjut Cara Kerja Scale Down

:image-display{src="/blog-img/idcloudhost-autoscaling/cara-kerja-scale-down-1.png"}

Ketika _request_ lebih rendah, ini akan memicu pemakaian sumber daya komputasional yang sedikit. Jika terdapat dua buah server, akan menjadi tidak efisien secara biaya untuk menjalankan dua buah server yang menangani beban yang bisa ditangani oleh satu server.

Berikut adalah tahapan yang akan dilakukan untuk melakukan scale down:

:image-display{src="/blog-img/idcloudhost-autoscaling/cara-kerja-scale-down-2.png"}

Proses yang terjadi sama dengan event scale up. Hanya saja kita akan melakukan penghapusan terhadap server clone dan menghapus koneksi yang ada di Traefik dan server monitoring.

Gambar di bawah ini adalah hasil dari event scale down:

:image-display{src="/blog-img/idcloudhost-autoscaling/cara-kerja-scale-down-3.png"}

1. Daftar server menyisakan service seed.
2. Koneksi server clone yang lama ke monitoring server telah dihapus.
3. Koneksi server clone yang lama ke Traefik telah dihapus.

## Menghindari Thrashing

### Menghindari Thrashing Karena Konfigurasi Threshold yang Tidak Optimal

Dalam menentukan ambang batas (threshold) kapan sistem harus melakukan scale-up dan scale-down, kita harus menentukannya dengan cermat. Jika tidak, dapat berisiko terjadi _thrashing_. _Thrashing_ adalah kondisi di mana server terus-menerus melakukan **scale-down** dan **scale-up** secara berulang tanpa henti. Kondisi ini dapat menyebabkan penggunaan sumber daya yang tidak efisien dan membebani sistem, seperti yang dijelaskan pada gambar di bawah.

:image-display{src="/blog-img/idcloudhost-autoscaling/thrashing.svg"}

1. Service clone dengan satu VM mengalami beban CPU 70%, yang melebihi ambang batas (threshold) untuk scale-up.
2. Sistem melakukan scale-up.
3. VM baru berhasil di-clone dan terhubung ke load balancer serta server monitoring.
4. Beban terbagi secara merata antara dua VM, sehingga rata-rata penggunaan CPU turun menjadi 35%, yang berada di bawah ambang batas untuk scale-down.
5. Sistem kemudian melakukan scale-down.
6. Beban CPU kembali naik ke 70%, menyebabkan sistem melakukan scale-up lagi. Kejadian ini terus berulang, menciptakan siklus yang disebut thrashing.

::info-box{type="info" title="Solusi"}
Aturan yang bisa diikuti adalah mengatur threshold scale-down setengah lalu dikurang 5 persen dari threshold scale-up. Jika threshold scale-up adalah 80%, maka threshold scale-down adalah 35%.
::

### Menghindari Thrashing Karena Volume Request yang Fluktuatif

Thrashing karena volume request yang fluktuatif disebabkan jika volume request melebihi threshold secara sementara dalam waktu yang singkat, seperti yang dijelaskan pada gambar di bawah:

:image-display{src="/blog-img/idcloudhost-autoscaling/graph-load-fluktuatif.png"}

Perhatikan _load_ sistem tiga kali berada di bawah 35%.

Jika pengaturan hanya menetapkan scale-down pada 35% tanpa kebijakan tambahan, maka scale-down akan terpicu meskipun beban sistem sebenarnya masih di atas threshold dalam kurun waktu menit. Hal ini dapat menyebabkan scale-down terjadi saat sistem sebenarnya belum membutuhkannya.

Untuk mengatasi masalah tersebut, dapat diterapkan **grace period** sebelum melakukan scale-down. Grace period ini memastikan bahwa sistem hanya melakukan scale-down jika beban tetap berada di bawah threshold selama jangka waktu tertentu, misalnya 5 menit berturut-turut.

Cara lain adalah menggunakan kalkulasi rata-rata beban selama 5 menit terakhir sebagai acuan untuk memicu scale-down, daripada langsung melakukan scale-down saat beban menyentuh threshold.

::info-box{type="info" title="Solusi"}
Untuk mengatasi thrashing akibat volume yang fluktuatif, kita dapat menerapkan grace period yang menunggu volume request melewati threshold selama kurun waktu tertentu sebelum memicu scaling. Kita juga bisa menggunakan kalkulasi rata-rata sebagai acuan untuk memicu scaling.
::

## Setup VPC Network

Pembuatan VPC Network pada IDCloudHost cukup simpel.

Pergi ke halaman network lalu buatlah network sesuai dengan kebutuhan seperti ini.
:image-display{src="/blog-img/idcloudhost-autoscaling/create-network.png"}

Lalu pastikan network tersebut merupakan _default network_. Jika belum, maka klik tombol "Set Default" pada halaman detail network.
:image-display{src="/blog-img/idcloudhost-autoscaling/set-default.png"}

## Setup Virtual Machine

Semua Virtual machine dibuat dengan spesifikasi berikut

- OS: Ubuntu 22.04 lts
- Processor: 2 VCPU
- RAM: 2048 MB
- Storage: 20 GB

## Implementasi VM Service

:image-display{src="/blog-img/idcloudhost-autoscaling/service-highlighted.png"}

VM Service adalah virtual machine (VM) yang menjalankan aplikasi utama.

VM Service Seed adalah VM yang akan menjadi bibit yang yang menjadi template untuk VM melakukan clone

Didalam VM Service terdapat dua buah aplikasi yang berjalan:

1. Service atau aplikasi itu sendiri
2. Prometheus client (Node Exporter): untuk mengumpulkan data sistem (CPU Usage, Memory Usage, dll) dan mengirimkannya ke prometheus yang ada di server monitoring.

Pada bagian ini kita akan melakukan

1. Setup Service
2. Setup Node Exporter

### Aplikasi yang akan dijalankan sebagai service

Service yang akan dijalankan pada Artikel ini adalah endpoint untuk melakukan hashing kriptografi (pbkdf2 dan scrypt).

Service ini menggunakan Node dan express yang memanfaatkan `worker_threads` dari Node sehingga endpoint dapat menggunakan semua thread yang ada di VM.

Endpoint ini intensif di komputasional jadi akan memakan banyak sumber daya CPU.

```js [index.js]
const express = require("express");
const crypto = require("crypto");
const {
  Worker,
  isMainThread,
  parentPort,
  workerData,
} = require("worker_threads");

const app = express();
const port = 3000;

app.use(express.json());

const performCryptoOperationsInWorker = (number) => {
  return new Promise((resolve, reject) => {
    const worker = new Worker(__filename, { workerData: number });

    worker.on("message", (result) => resolve(result));
    worker.on("error", (error) => reject(error));
  });
};

if (!isMainThread) {
  const { number } = workerData;

  const performCryptoOperations = (number) => {
    const password = "complex_password";
    const salt = `unique_salt_${number}`;
    const iterations = 1000; // Sepuluh ribu - tingkatkan jika ingin beban komputasional lebih
    const keyLength = 64;

    const pbkdf2Hash = crypto
      .pbkdf2Sync(password, salt, iterations, keyLength, "sha512")
      .toString("hex");
    const scryptHash = crypto
      .scryptSync(password, salt, keyLength, {
        N: 2 ** ((number % 14) + 1),
        r: 8,
        p: 1,
      })
      .toString("hex");

    return { pbkdf2Hash, scryptHash };
  };

  const result = performCryptoOperations(workerData);
  parentPort.postMessage(result);
} else {
  app.post("/calculate", async (req, res) => {
    const { number } = req.body;

    if (typeof number !== "number" || number <= 0) {
      return res.status(400).json({ error: "Input harus bilangan positif" });
    }

    try {
      const result = await performCryptoOperationsInWorker(number);
      res.json({
        input: number,
        ...result,
      });
    } catch (error) {
      res
        .status(500)
        .json({ error: "An error occurred while processing the request" });
    }
  });

  app.listen(port, () => {
    console.log(`Server running at http://localhost:${port}`);
  });
}
```

### Prometheus Client (Node Exporter)

Berikut adalah tahapan untuk menginstall dan mengkonfigurasi Node Exporter:

1. Lihat halaman ini https://prometheus.io/download/#node_exporter untuk mengetahui versi terbaru dari Node Exporter

2. Instal versi terbaru Node Exporter menggunakan `wget`

   ```bash [bash]
   wget https://github.com/prometheus/node_exporter/releases/download/v1.8.2/node_exporter-1.8.2.linux-amd64.tar.gz
   ```

   Cukup ganti `1.8.2` dengan versi terbaru

3. Ekstrak aplikasi.

   ```bash [bash]
   tar xvfz node_exporter-*.tar.gz
   ```

4. Pindahkan executable `node_exporter` kedalam folder `usr/local/bin` sehingga bisa diakses seluruh sistem

   ```bash [bash]
   sudo mv node_exporter-1.8.2.linux-amd64/node_exporter /usr/local/bin
   ```

5. (**Opsional**) Hapus sisa file

   ```bash [bash]
   rm -r node_exporter-1.8.2.linux-amd64*
   ```

6. Jalankan command berikut untuk melihat status instalasi node_exporter

   ```bash [bash]
   node_exporter
   ```

   Node Exporter berhasil diinstal jika muncul pesan seperti ini

   ```
   ts=2024-10-22T12:55:16.946Z caller=node_exporter.go:193 level=info msg="Starting node_exporter" version="(version=1.8.2, branch=HEAD, revision=f1e0e8360aa60b6cb5e5cc1560bed348fc2c1895)"
   Dan seterusnya ...
   ```

#### Menjalankan Node Exporter sebagai _systemd_

Node Exporter telah berhasil diinstal, dan sudah dapat dijalankan. Namun, untuk memastikan Node Exporter otomatis berjalan saat server dinyalakan, Node Exporter perlu dijalankan sebagai _system service_. Dengan begitu, ketika server di-clone, Node Exporter akan langsung aktif tanpa perlu dimulai secara manual.

1. Pertama buatlah user untuk node exporter

   ```bash [bash]
   sudo useradd -rs /bin/false node_exporter
   ```

2. Buatlah service file yang akan digunakan oleh `systemctl`. File harus bernama`node_exporter.service`

   ```bash [bash]
   sudo nano /etc/systemd/system/node_exporter.service
   ```

   Isi dari file adalah sebagai berikut:

   ```[/etc/systemd/system/node_exporter.service]
   [Unit]
   Description=Node Exporter
   Wants=network-online.target
   After=network-online.target

   [Service]
   User=node_exporter
   Group=node_exporter
   Type=simple
   Restart=on-failure
   RestartSec=5s
   ExecStart=/usr/local/bin/node_exporter

   [Install]
   WantedBy=multi-user.target
   ```

3. Untuk menjalankan Node Exporter secara otomatis ketika server dimulai (boot time), jalankan command `systemctl enable`

   ```bash [bash]
   sudo systemctl enable node_exporter
   ```

4. Reload `systemctl` daemon, Mulai node_exporter dan cek statusnya

   ```bash [bash]
   sudo systemctl daemon-reload
   sudo systemctl start node_exporter
   sudo systemctl status node_exporter
   ```

   Jika Node Exporter telah aktif maka akan mengeluarkan pesan seperti ini:

   ```bash [bash]
   node_exporter.service - Node Exporter
   Loaded: loaded (/etc/systemd/system/node_exporter.service; enabled; vendor preset: enabled)
   Active: active (running) since Tue 2023-04-11 13:48:06 UTC; 4s ago
   ```

5. Cek url http://localhost:9100/metrics untuk melihat apakah node exporter sudah berjalan

   ```bash [bash]
   curl http://localhost:9100/metrics
   ```

## Implementasi Server Traefik API Gateway dan Load Balancer

:image-display{src="/blog-img/idcloudhost-autoscaling/traefik-highlighted.png"}
Terdapat dua aplikasi yang berjalan pada server ini yaitu Traefik dan API Target Updater

Traefik akan digunakan sebagai load balancer dan API target updater adalah endpoint yang akan mengupdate daftar server yang terhubung ke Traefik. API target updater akan digunakan oleh webhook handler pada server monitoring setelah ada server yang berhasil ditambah atau dihapus.

Bagian ini akan menjelaskan tentang:

1. Instalasi dan konfigurasi Traefik
2. Pembuatan API target updater berupa server Express

### Instalasi Traefik

1. Install Traefik binary

   cek versi traefik terbaru di https://github.com/traefik/traefik/releases

   ```bash [bash]
    wget https://github.com/traefik/traefik/releases/download/v3.1.4/traefik_v3.1.4_linux_amd64.tar.gz
   ```

   Ganti v3.1.4 dengan versi terbaru

2. Ekstrak file Traefik

   ```bash [bash]
   tar -zxvf traefik_v3.1.4_linux_amd64.tar.gz
   ```

3. Pindahkan file bernama 'traefik' ke usr/local/bin

   ```bash [bash]
   sudo mv traefik /usr/local/bin
   ```

4. Cek status instalasi Traefik
   ```bash [bash]
   traefik --help
   ```

### Menulis File Konfigurasi Untuk Traefik

Pada bagian ini kita akan membuat file konfigurasi yang diperlukan untuk menjalankan Traefik

Berikut cara kerjanya

:image-display{src="/blog-img/idcloudhost-autoscaling/traefik-file.png"}

1. `static.yml`, berisi konfigurasi dasar yaitu entrypoints, dan letak file konfigurasi
2. `configs/dynamic.yaml`, berisikan endpoint yang akan diekspos ke publik dan server apa yang menangani endpoint tersebut dan pengaturan lain.
3. `configs/services.yaml`, beriskan daftar server yang terhubung dengan Traefik.

File `static.yml` akan digunakan oleh Traefik sebagai file konfigurasi utama. Lalu file ini akan terhubung dengan folder `configs` yang berisikan dua buah file konfigurasi dinamis yaitu `dynamic.yaml` dan `services.yaml`. Kegunaaan dari setiap file akan dijelaskan pada bagiannya masing-masing.

1. Buat folder configs

Buatlah folder traefik didalam folder `/etc`, folder ini akan menjadi wadah file konfigurasi yang akan digunakan traefik.

semua file yang berkaitan dengan Traefik akan diletakan disini.

```bash [bash]
sudo mkdir /etc/traefik
```

2. Pembuatan File `/etc/traefik/static.yaml`

```bash [bash]
sudo nano /etc/traefik/static.yaml
```

```yaml [/etc/traefik/static.yaml]
entryPoints:
  web:
    address: ":80"

providers:
  file:
    directory: /etc/traefik/configs
    watch: true
```

Diatas adalah konfigurasi minimal yang digunakan untuk menjalankan Traefik dengan file provider.
Konfigurasi ini mendefinisikan **entryPoint** bernama `web` pada port `80`, Karena yagn diekspos adalah port 80 artinya Traefik akan menerima _request_ berjenis http.

Bagian **providers** menunjukan letak tempat file konfigurasi disimpan.
Opsi `watch: true` memungkinkan Traefik untuk secara otomatis memantau perubahan pada file konfigurasi di direktori tersebut dan menerapkan pembaruan tanpa memerlukan restart.

2. Pembuatan File Dynamic config `/etc/traefik/configs/dynamic.yaml`

Buat folder configs untuk menampung file konfigurasi dinamis yang akan digunakan file konfigurasi.

```bash [bash]
sudo mkdir /etc/traefik/configs
```

Buat file konfigurasi `dynamic.yaml`

```bash [bash]
sudo nano /etc/traefik/configs/dynamic.yaml
```

Isi dengan kode berikut:

```yaml
http:
  routers:
    calculate:
      entrypoints:
        - "web"
      rule: "Path(`/calculate`)"
      service: "calculate"
```

Kode diatas mendefinisikan sebuah router untuk menangani permintaan HTTP ke path /calculate. Penjelasan singkatnya:

- routers.calculate: Mendefinisikan router bernama calculate.
- entrypoints: Menunjukkan bahwa router ini akan menangani request yang masuk melalui entrypoint web (port 80 untuk HTTP).
- rule dan service: service calculate akan dijalankan ketika user melakukan request ke `/calculate`

3. Pembuatan File Services config `/etc/traefik/configs/services.yaml`

File ini mendifinisikan apa itu service `calculate`.

```bash [bash]
sudo nano /etc/traefik/configs/services.yaml
```

```yaml
http:
  services:
    calculate:
      loadBalancer:
        servers:
          - url: "http://<private-ip-services>:3000/"
          - url: "http://<private-ip-services>:3000/"
```

- `services.calculate`: Mendefinisikan sebuah service bernama `calculate`.
- `loadBalancer`: Menggunakan load balancer untuk mendistribusikan permintaan ke daftar server yang ada di bagian `servers`.
- `servers`: Mendefinisikan daftar url server backend yang akan menerima permintaan.
- `url`: Menunjukkan URL milik server service.

Request akan didistribusikan kepada daftar Url dengan metode round robin

File ini akan diupdate setiap kali event scaling terjadi, untuk itu buka akses `read` dan `write` pada file ini sehingga daftar url bisa diupdate secara otomatis oleh sistem.

```bash [bash]
sudo chmod +rw /etc/traefik/configs/services.yaml
```

### Menjalankan Traefik dengan systemd

Aplikasi akan dijalankan dengan systemd untuk memanfaatkan fitur startup otomatis saat reboot.

Pertama buatlah user Traefik:

```bash [bash]
sudo useradd -rs /bin/false traefik
```

Ubah izin file Traefik agar bisa dieksekusi oleh user Traefik.

```bash [bash]
sudo chmod 755 /usr/local/bin/traefik
```

Berikan izin kepada traefik untuk menggunakan ports yang "privileged" (80, 443) sebagai non-root user

```bash [bash]
sudo setcap 'cap_net_bind_service=+ep' /usr/local/bin/traefik
```

Lalu buat file service:

```bash [bash]
sudo nano /etc/systemd/system/traefik.service
```

Berikut isinya

```
[Unit]
Description=Traefik
Wants=network-online.target
After=network-online.target

[Service]
ExecStart=/usr/local/bin/traefik --configFile=/etc/traefik/static.yaml
User=traefik
Group=traefik
Restart=on-failure
RestartSec=5s

[Install]
WantedBy=multi-user.target
```

Enable service traefik sehingga dapat mulai secara otomatis jika vm reboot

```bash [bash]
sudo systemctl enable traefik
```

Reload daemon, mulai traefik dan cek status:

```bash [bash]
sudo systemctl daemon-reload
sudo systemctl start traefik
sudo systemctl status traefik
```

### Pembuatan Target Updater

Bagian ini akan memberikan arahan cara membuat sebuah server node menggunakan express untuk menangangi event scale down dan event scale up

Tugas dari sistem ini adalah untuk melakukan update ke file `/etc/traefik/config/services.yaml` untuk memperbarui daftar server pada `loadBalancer.servers`.

#### Pembuatan service express untuk menerima daftar server terbaru

1. Install Node

   Berikut adalah langkah-langkah menginstall Node v20 pada Linux menggunakan nvm diambil dari [website resmi Node](https://nodejs.org/en/download/package-manager)

   ```bash [bash]
   curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.0/install.sh | bash

   ##### REFRESH TERMINAL #####

   nvm install 20

   node -v

   npm -v
   ```

2. Inisiasi Folder API

   Buat sebuah folder untuk API yang akan dibuat, pada artikel ini folder tersebut akan diberi nama `server-list-updater`

   ```bash [bash]
   mkdir server-list-updater
   cd server-list-updater

   npm init -y
   npm install express
   ```

3. Pembuatan Endpoint
   Buatlah file `index.js`

   ```bash [bash]
   touch index.js
   ```

   Lalu tulis kode berikut:

   ```js
   const express = require("express");
   const fs = require("fs");
   const path = require("path");
   const YAML = require("yaml"); // Library untuk mengonversi objek JavaScript ke format YAML

   const app = express();
   const PORT = 3000;
   const servicesFilePath = "/etc/traefik/configs/services.yaml";

   app.use(express.json());

   // Endpoint untuk memperbarui daftar server pada file services.yaml
   // Endpoint menerima parameter berupa array of string yang berisi daftar url services terbaru
   app.post("/update-servers", (req, res) => {
     const { servers } = req.body;

     // Validasi input: Pastikan 'servers' adalah array yang tidak kosong
     if (!servers || !Array.isArray(servers) || servers.length === 0) {
       return res.status(400).json({
         message:
           "Invalid request body. Expected a non-empty array of servers.",
       });
     }

     // Struktur data YAML yang akan ditulis ke file services.yaml
     // Object ini akan diubah oleh library YAML ke bentuk string yaml

     const servicesYamlContent = {
       http: {
         services: {
           mathCalculate: {
             loadBalancer: {
               // mengisi daftar url dengan url terbaru yang didapat dari body
               servers: servers.map((url) => ({ url })),
             },
           },
         },
       },
     };

     try {
       // mengubah object javascript ke string yaml
       const yamlString = YAML.stringify(servicesYamlContent);
       fs.writeFileSync(servicesFilePath, yamlString);

       return res
         .status(200)
         .json({ message: "services.yaml updated successfully" });
     } catch (error) {
       console.error("Error writing services.yaml file:", error);
       return res.status(500).json({
         message: "Failed to update services.yaml",
         error: error.message,
       });
     }
   });

   app.listen(PORT, () => {
     console.log(`Server is running on port ${PORT}`);
   });
   ```

#### Menjalankan API Target Updater Dengan PM2

::info-box{type="info" title="PM2"}
PM2 adalah process manager untuk aplikasi Node.js yang berfungsi mengelola, memonitor, dan menjalankan aplikasi secara otomatis dalam lingkungan produksi.
::

1. Install PM2:

   ```bash [bash]
   npm install pm2 -g
   ```

2. Jalankan aplikasi server target udpater

   ```bash [bash]
   pm2 start index.js --name server-list-updater
   ```

3. Setelah menjalankan index.js, simpan daftar service yang akan dijalankan saat server reboot:

   ```bash [bash]
   pm2 save
   ```

4. Jalankan perintah `pm2 startup` dan copy/paste command yang diberikan

   ```bash [bash]
   pm2 startup
     [PM2] Init System found: systemd
     [PM2] To setup the Startup Script, copy/paste the following command:
     sudo env PATH=$PATH:/home/falestio/.nvm/versions/node/v20.18.0/bin /home/falestio/.nvm/versions/node/v20.18.0/lib/node_modules/pm2/bin/pm2 startup systemd -u falestio --hp /home/falestio
   ```

   Jalankan perintah yang diberikan

   ```bash [bash]
   sudo env PATH=$PATH:/home/falestio/.nvm/versions/node/v20.18.0/bin /home/falestio/.nvm/versions/node/v20.18.0/lib/node_modules/pm2/bin/pm2 startup systemd -u falestio --hp /home/falestio
   ```

   Sekarang service `server-list-updater` akan tetap berjalan meskipun server di reboot.

#### Tes Fungsi API

1. Jalankan command curl berikut

   ```bash [bash]
   curl -X POST http://localhost:3000/update-servers \
   -H "Content-Type: application/json" \
   -d '{
     "servers": [
       "http://192.168.1.101:8080",
       "http://192.168.1.102:8080"
     ]
   }'
   ```

2. Cek apakah daftar url sudah berubah

   ```bash [bash]
   cat /etc/traefik/configs/services.yaml
   ```

## Implementasi Server Monitoring

:image-display{src="/blog-img/idcloudhost-autoscaling/monitoring-highlighted.png"}

Server ini akan menjalankan Grafana dan Prometheus seabgai tools untuk mengumpulkan data dari server service, menvisualisasikan data tersebut, dan menginisiasi scaling ketika load sistem telah melewati batas yang ditentukan.

Bagian ini akan menjelaskan tentang:

1. Instalasi dan Konfigurasi Prometheus
2. Instalasi dan Konfigurasi Grafana
3. Menghubungkan Prometheus dan Grafana
4. Setup Dashboard Grafana

::info-box{type="info" title="Sumber"}
Panduan instalasi Prometheus dan Grafana pada artikel ini didapat dari: https://www.linode.com/docs/guides/how-to-install-prometheus-and-grafana-on-ubuntu/#how-to-configure-prometheus-as-a-service
::

### Download dan install prometheus

Untuk menginstall Prometheus, ikuti langkah-langkah berikut.

1. Cek link berikut untuk mengetahui versi terbaru dari prometheus https://github.com/prometheus/prometheus/releases

   ```bash [bash]
   wget https://github.com/prometheus/prometheus/releases/download/v2.55.0/prometheus-2.55.0.linux-amd64.tar.gz
   ```

   Ganti v2.55.0 dengan versi terbaru.

2. Ekstrak file Prometheus

   ```bash [bash]
   tar xvfz prometheus-*.tar.gz
   ```

3. (Opsional) Setelah file diekstrak, hapus arsip atau pindahkan ke lokasi lain untuk penyimpanan.

   ```bash [bash]
   rm prometheus-*.tar.gz
   ```

4. Buat dua direktori baru yang akan digunakan oleh Prometheus. Direktori `/etc/prometheus` menyimpan file konfigurasi Prometheus. Direktori `/var/lib/prometheus` menyimpan data aplikasi.

   ```bash [bash]
   sudo mkdir /etc/prometheus /var/lib/prometheus
   ```

5. Masuk ke folder prometheus yang baru diekstrak.

   ```bash [bash]
   cd prometheus-2.55.0.linux-amd64
   ```

6. Pindahkan direktori `prometheus` dan `promtool` ke direktori `/usr/local/bin/`. Ini membuat Prometheus dapat diakses oleh semua pengguna.

   ```bash [bash]
   sudo mv prometheus promtool /usr/local/bin/
   ```

7. Pindahkan file konfigurasi YAML `prometheus.yml` ke direktori `/etc/prometheus`.

   ```bash [bash]
   sudo mv prometheus.yml /etc/prometheus/prometheus.yml
   ```

   pindahkan juga folder `consoles` dan `console_libraries`. fodler tersebut tidak dibahas dalam artikel ini. Namun, file-file ini juga harus dipindahkan ke direktori `/etc/prometheus` jika suatu saat diperlukan.

   ```bash [bash]
   sudo mv consoles/ console_libraries/ /etc/prometheus/
   ```

   Setelah direktori ini dipindahkan, hanya file `LICENSE` dan `NOTICE` yang tersisa di direktori asli.

8. Verifikasi bahwa Prometheus berhasil diinstal dengan menggunakan perintah berikut:

   ```bash [bash]
   prometheus --version
   ```

   Contoh output:

   ```bash [bash]
   prometheus, version 2.55.0 (branch: HEAD, revision: 9sda30252c3e528722890348das8dasdd720f6be07cb8)
     build user:       root@9fad779131cc
     build date:       20241022-13:47:22
     go version:       go1.23.2
     platform:         linux/amd64
     tags:             netgo,builtinassets,stringlabels
   ```

### Menjalankan Prometheus Sebagai Systemd

Meskipun Prometheus bisa dijalankan dan dihentikan langsung dari command line, lebih praktis kalau dijalankan sebagai service menggunakan systemd.

1. Buat user untuk Prometheus. Perintah berikut akan membuat user sistem:

   ```bash [bash]
   sudo useradd -rs /bin/false prometheus
   ```

2. Ubah kepemilikan dua direktori yang sudah dibuat sebelumnya ke user `prometheus` yang baru.

   ```bash [bash]
   sudo chown -R prometheus: /etc/prometheus /var/lib/prometheus
   ```

3. Untuk menjalankan Prometheus sebagai service, buat file `prometheus.service` dengan perintah ini:

   ```bash [bash]
   sudo nano /etc/systemd/system/prometheus.service
   ```

   Lalu masukan kode berikut:

   ```
   [Unit]
   Description=Prometheus
   Wants=network-online.target
   After=network-online.target

   [Service]
   User=prometheus
   Group=prometheus
   Type=simple
   Restart=on-failure
   RestartSec=5s
   ExecStart=/usr/local/bin/prometheus \
       --config.file /etc/prometheus/prometheus.yml \
       --storage.tsdb.path /var/lib/prometheus/ \
       --web.console.templates=/etc/prometheus/consoles \
       --web.console.libraries=/etc/prometheus/console_libraries \
       --web.listen-address=0.0.0.0:9090 \
       --web.enable-lifecycle \
       --log.level=info

   [Install]
   WantedBy=multi-user.target
   ```

4. Setelah itu, reload daemon:

   ```bash [bash]
   sudo systemctl daemon-reload
   ```

5. Gunakan perintah `systemctl enable` agar Prometheus otomatis mulai saat sistem di-boot.

   ```bash [bash]
   sudo systemctl enable prometheus
   ```

6. Jalankan service Prometheus dan cek statusnya untuk memastikan sudah aktif.

   ```bash [bash]
   sudo systemctl start prometheus
   sudo systemctl status prometheus
   ```

7. Akses interface web dan dashboard Prometheus di `http://local_ip_addr:9090`. Ganti `local_ip_addr` dengan alamat IP server monitoring. Karena menggunakan file konfigurasi default, informasi yang ditampilkan masih terbatas.

### Edit file Konfigurasi

1. Edit file konfigurasi prometheus

   Pada server monitoring yang menjalankan Prometheus, buka file `prometheus.yml` untuk diedit.

   ```bash [bash]
   sudo nano /etc/prometheus/prometheus.yml
   ```

   isi dengan kode berikut:

   ```yaml
   global:
     scrape_interval: 5s

   scrape_configs:
     - job_name: "calculate_service_clones"
       file_sd_configs:
         - files:
             - "targets.json"

     - job_name: "calculate_service_seed"
       static_configs:
         - targets: ["<private_ip>:9100"]
   ```

   - `scrape_interval`: Interval waktu (5 detik) untuk Prometheus mengambil data dari target.
   - `job_name: "calculate_service_clones"`: ini adalah job untuk mengumpulkan data sistem dari server clone. Daftar server yang akan di clone berada pada file targets.json. ini dilakukan karena daftar server bersifat dinamis (selalu berubah).
   - `job_name: "calculate_service_seed"`: mengarah ke service seed, service seed akan selalu ada jadi menggunakan `static_configs`

2. Buatlah file targets.json

   ```bash [bash]
   sudo nano /etc/prometheus/targets.json
   ```

   Lalu cukup isi file dengan array kosong `[]`.

   File ini akan diupdate oleh prometheus target updater, untuk permissions dari file perlu di modifikasi sehingga target updater (express) dapat melakukan perubahan kepada file.

   ```bash [bash]
   sudo chmod -R a+rwx /etc/prometheus/targets.json
   ```

3. Restart Prometheus dan cek kembali `http://local_ip_addr:9090`

   ```bash [bash]
   sudo systemctl restart prometheus
   ```

::info-box{type="info" title="Memonitor API gateway"}
Anda bisa mengimplementasikan monitoring terhadap server api gateway jika diperlukan, namun agar sistem autoscaling berjalan anda tidak perlu melakukan itu
::

### Pembuatan Webhook Handler

Webhook handler adalah endpoint yang akan dipanggil oleh Grafana Alert ketika salah satu proses scaling terjadi. Sistem ini akan berinteraksi dengan IDCloudHost API untuk melakukan clone pada server seed dan mengupdate daftar target di Server Traefik dan Server monitoring.

Webhook hanler berbentuk server Node dengan menggunakan library Express.

:image-display{src="/blog-img/idcloudhost-autoscaling/webhook-handler-highlighted.png"}

1.  Buat API Token untuk mengakses IDCloudHost API:

    Pergi ke halaman access lalu klik tombol "Create New Access".

    :image-display{src="/blog-img/idcloudhost-autoscaling/create-new-access.png"}

    Pilih tipe "API Token", sesuaikan scope, billing account, dan access name.

    Simpan API Token untuk digunakan membuat webhook handler.

    :image-display{src="/blog-img/idcloudhost-autoscaling/api-token.png"}

2.  Buat folder webhook handler dan inisiasi proyek node

    ```bash [bash]
    sudo mkdir webhook-handler
    npm init -y
    ```

3.  Install library yang diperlukan

    ```bash [bash]
    npm install express axios
    ```

4.  Isi index.js dengan kode berikut

    ```js [prometheus-target-updater/index.js]
    const express = require("express");
    const axios = require("axios");
    const { exec } = require("child_process");
    const app = express();
    const fs = require("fs");

    // ========= SESUAIKAN BAGIAN INI - GUNAKAN ENV ATAU SECRET MANAGER LAIN =========
    // API Token yang dibuat pada bagian ini
    const apiKey = "IBlVjA6KTfJlH5nYO79mAQBwYCxGhv71";

    // UUID dari private network yang dibuat pada awal artikel
    const privateNetworkUuid = "b34ec3de-76ef-4720-8737-7662f52f6f89";

    // UUID dari service seed
    const serviceSeedUuid = "a0c79e13-6eda-4e68-a64c-91637589ec55";

    // pergi ke halaman billing dna klik pada detail salah satu billing account
    const billingAccountId = "1200211740";

    // nama pertama dari service clone, dingunakan untuk menetukan nama server dari service clone
    // yaitu: calculate-service-clone-{nomorurut}
    const serviceChildFirstName = "calculate-service-clone-";

    // file path ke file targets.json yang berisi daftar server yang terhubung ke server monitoring
    const prometheusTargetsFilePath = "/etc/prometheus/targets.json";

    // Url dari target updater yang ada di server Traefik
    const updateTraefikTargetsUrl = "http://10.52.76.11:3000/update-servers";

    // Port dimana aplikasi service dijalankan di server service
    const servicePort = 3000;
    // =====================================================================

    app.use(express.json());

    // Fetching data semua VM service
    const getServiceVmsData = async (filterPrefix = "calculate-service-") => {
      try {
        const { data } = await axios.get(
          "https://api.idcloudhost.com/v1/jkt01/user-resource/vm/list",
          { headers: { apikey: apiKey } }
        );
        return data.filter((vm) => vm.name.startsWith(filterPrefix));
      } catch (error) {
        console.error("Error fetching VM data:", error.message);
        throw error;
      }
    };

    // clone terhadap service seed server
    const cloneVM = async (name) => {
      const { data } = await axios.post(
        `https://api.idcloudhost.com/v1/jkt01/user-resource/vm/clone?uuid=${serviceSeedUuid}&name=${name}`,
        {},
        { headers: { apikey: apiKey } }
      );
      return data;
    };

    // hapus public ip dari VM yang baru di Clone
    const deletePublicIp = async (vmUuid) => {
      const { data } = await axios.get(
        `https://api.idcloudhost.com/v1/jkt01/network/ip_addresses?billing_account_id=${billingAccountId}`,
        { headers: { apikey: apiKey } }
      );
      const ip = data.find((ip) => ip.assigned_to === vmUuid);
      if (ip) {
        await axios.delete(
          `https://api.idcloudhost.com/v1/jkt01/network/ip_addresses/${ip.address}`,
          { headers: { apikey: apiKey } }
        );
      }
    };

    // update daftar target pada server service dan prometheus
    const updateTargets = async (type, data) => {
      if (type === "prometheus") {
        const prometheusTargets = [
          {
            labels: {
              job: "calculate_service_clone",
            },
            targets: data.map((ip) => `${ip}:9100`),
          },
        ];

        try {
          fs.writeFileSync(
            prometheusTargetsFilePath,
            JSON.stringify(prometheusTargets, null, 2)
          );
          console.log("Prometheus targets updated");
        } catch (err) {
          console.error("Error updating Prometheus targets:", err.message);
          throw err;
        }
      } else if (type === "traefik") {
        try {
          const servers = data.map((ip) => `http://${ip}:${servicePort}`);
          const response = await axios.post(
            updateTraefikTargetsUrl,
            { servers },
            { headers: { "Content-Type": "application/json" } }
          );
          console.log("Traefik targets updated:", response.data);
        } catch (error) {
          console.error("Error updating Traefik targets:", error.message);
          throw error;
        }
      }
    };

    const scaleUpHandler = async (req, res) => {
      try {
        const serviceVms = await getServiceVmsData();
        const newVmData = await cloneVM(
          `${serviceChildFirstName}${serviceVms.length}`
        );
        await deletePublicIp(newVmData.uuid);

        const updatedVms = await getServiceVmsData(serviceChildFirstName);
        const privateIps = updatedVms.map((vm) => vm.private_ipv4);

        await updateTargets("prometheus", privateIps);
        await updateTargets("traefik", privateIps);

        res.send({ newVmData });
      } catch (error) {
        res.status(500).send({ error: error.message });
      }
    };

    const scaleDownHandler = async (req, res) => {
      try {
        const vmList = await getServiceVmsData(serviceChildFirstName);
        if (!vmList.length)
          return res.send({ message: "No child VM, scale down aborted" });

        const vmToDelete = vmList.sort(
          (a, b) => new Date(b.created_at) - new Date(a.created_at)
        )[0];

        await axios.delete(
          `https://api.idcloudhost.com/v1/jkt01/user-resource/vm?uuid=${vmToDelete.uuid}`,
          { headers: { apikey: apiKey } }
        );

        const updatedVms = await getServiceVmsData(serviceChildFirstName);
        const privateIps = updatedVms.map((vm) => vm.private_ipv4);

        await updateTargets("prometheus", privateIps);
        await updateTargets("traefik", privateIps);

        res.send({ message: "VM Deleted", vmDeleted: vmToDelete });
      } catch (error) {
        res.status(500).send({ error: error.message });
      }
    };

    app.post("/v1/scale-up", scaleUpHandler);
    app.post("/v1/scale-down", scaleDownHandler);

    const PORT = 4000;
    app.listen(PORT, () => console.log(`Server running on port ${PORT}`));
    ```

    Jalankan Webhook handler menggunakan PM2, Panduan bisa dilihat diatas.

5.  Test endpoint webhook handler

    ```bash [bash]
    curl -X POST http://localhost:4000/v1/scale-up
    ```

    ```bash [bash]
    curl -X POST http://localhost:4000/v1/scale-down
    ```

### Install Grafana

Sekarang Prometheus mengumpulkan statistik dari klien yang terdaftar di bagian `scrape_configs` pada file konfigurasinya. Namun, informasi tersebut hanya dapat dilihat dalam bentuk data mentah yang sulit dibaca dan kurang berguna.

Grafana menyediakan antarmuka untuk melihat statistik yang dikumpulkan oleh Prometheus. Grafana juga akan digunakan sebagai

1. Instal beberapa utilitas yang dibutuhkan menggunakan apt.

   ```bash [bash]
   sudo apt-get install -y apt-transport-https software-properties-common
   ```

2. Impor kunci GPG Grafana.

   ```bash [bash]
   sudo wget -q -O /usr/share/keyrings/grafana.key https://apt.grafana.com/gpg.key
   ```

3. Tambahkan repository "stable releases" Grafana.

   ```bash [bash]
   echo "deb [signed-by=/usr/share/keyrings/grafana.key] https://apt.grafana.com stable main" | sudo tee -a /etc/apt/sources.list.d/grafana.list
   ```

4. Update paket, termasuk paket Grafana yang baru.

   ```bash [bash]
   sudo apt-get update
   ```

5. Install versi open-source dari Grafana.

   ```bash [bash]
   sudo apt-get install grafana
   ```

6. Muat ulang daemon systemctl.

   ```bash [bash]
   sudo systemctl daemon-reload
   ```

7. Aktifkan dan mulai server Grafana. Menggunakan `systemctl enable`.

   ```bash [bash]
   sudo systemctl enable grafana-server.service
   sudo systemctl start grafana-server
   ```

8. Periksa status server Grafana.

   ```bash [bash]
   sudo systemctl status grafana-server
   ```

   ```
   grafana-server.service - Grafana instance
   Loaded: loaded (/lib/systemd/system/grafana-server.service; enabled; vendor preset: enabled)
   Active: active (running) since Tue 2023-04-11 17:31:53 UTC; 9s ago
   ```

### Menambahkan Prometheus Sebagai Sumber Data

Semua komponen sistem sekarang sudah terinstal, tetapi Grafana dan Prometheus belum diatur untuk berinteraksi. Untuk menambahkan Prometheus sebagai sumber data dan konfigurasi lainnya, dapat dilakukan menggunakan antarmuka web Grafana.

Untuk mengintegrasikan Grafana dan Prometheus, ikuti langkah-langkah di bawah ini:

1. Login ke grafana

   Dengan menggunakan browser, pergi ke halaman http://<public-up-server-monitoring>:3000

   Login ke grafana menggunakan default password dan username yaitu

   Username: admin
   Password: admin

   Setelah itu Grafana akan meminta kita untuk mengubah kata sandi. Setelah berhasil mengubah kata sandi, Grafana akan menampilkan **Grafana Dashboard**.

2. Tambah Prometheus sebagai sumber data

   Pada menu connection pergi ke halaman data source lalu klik add data source

   :image-display{src="/blog-img/idcloudhost-autoscaling/add-data-source.png"}

   Pilih **Prometheus** sebagai data source.

   :image-display{src="/blog-img/idcloudhost-autoscaling/pilih-prometheus.png"}

   Karena server prometheus sudah berjalan pada server yang sama, isi bagian connection dengan url `http://localhost:9090`

   :image-display{src="/blog-img/idcloudhost-autoscaling/pilih-prometheus.png"}

   Pengaturan lainnya bisa ditinggalkan menggunakan yang default

   Klik "Save & test", Jika berhasil Prometheus akan mengeluarkan pesan sukses

   :image-display{src="/blog-img/idcloudhost-autoscaling/pesan-sukses-prometheus.png"}

### Cara Mengimpor Dashboard Grafana

Dashboard menampilkan statistik dari data source menggunakan diagram, grafik, dengan tata letak yang efisien dan mudah dipahami. Anda bisa membuat dashboard kustom, tetapi Prometheus telah menyediakan dashboard yang mendukung data dari **Node Exporter**. Dashboard **Node Exporter Full** menggambarkan sebagian besar nilai yang dikumpulkan dari node klien. Lebih mudah mengimpor dashboard ini daripada membuat yang kustom.

Untuk mengimpor dashboard **Node Exporter**, ikuti langkah-langkah di bawah ini:

1. Pergi kehalaman dashboard, klik "+ Create dashboard"

:image-display{src="/blog-img/idcloudhost-autoscaling/import-dashboard-grafana.png"}

Lalu klik tombol "Import dashboard"

2. Pada halaman import dashboard masukan id Node Exporter Full dan klik "Load"

   :image-display{src="/blog-img/idcloudhost-autoscaling/import-dashboard-id.png"}

3. Beri nama dashboard dan pilih Prometheus sebagai datasource lalu klik import

   :image-display{src="/blog-img/idcloudhost-autoscaling/import-dashboard-pick-datasource.png"}

4. Jika dashboard berhasil dikonfigurasi maka akan memunculkan semua matrik yang disajikan oleh Node Exporter

   :image-display{src="/blog-img/idcloudhost-autoscaling/grafana-dashboard-finish.png"}

## Implementasi Alerting Menggunakan Grafana Alert

Grafana memiliki fitur alerting. Pada bagian ini kita akan mendefiniskan alert rules beserta apa yang harus dilakukan ketika alert rules tersebut terpenuhi.

### Menentukan parameter yang tepat sebagai acuan untuk scaling

Kita bisa menggunakan data penggunaan CPU, penggunaan RAM, atau gabungan keduanya sebagai acuan untuk scaling. Namun, setiap aplikasi memiliki karakteristik beban yang berbeda, ada yang lebih banyak menggunakan CPU dan ada yang lebih banyak menggunakan RAM.

Untuk mengetahui karakteristik aplikasi, kita dapat melakukan load testing menggunakan tools seperti Apache JMeter, Locust, atau k6. Tujuan dari pengujian ini adalah untuk mengidentifikasi _bottleneck_ dalam sistem, apakah disebabkan oleh CPU atau RAM. Setelah _bottleneck_ teridentifikasi, metrik tersebut dapat digunakan sebagai acuan untuk scaling.

Pada artikel ini apliaksi yang digunakan adalah CPU intensive. Jadi akan menggunakan query untuk mengmbil data Penggunaan CPU.

### Query PromQL Untuk Mendapatkan Data Rata-rata Penggunaan CPU dan RAM

Berikut adalah query untuk mengumpulkan data penggunaan CPU, penggunaan RAM, atau gabungan keduanya.

Query ini akan digunakan pada bagian selanjutnya saat mengkonfigurasi alert rule.

#### Menghitung Rata-rata CPU Load

```
100 - (avg(
 rate(node_cpu_seconds_total{job=~"calculate_service_seed|calculate_service_clone", mode="idle"}[2m])
)*100)
```

#### Menghitung Rata-rata Memory used

```
100 * (1 - (
  sum(avg_over_time(node_memory_MemAvailable_bytes{job=~"calculate_service_seed|calculate_service_clone"}[3m]))
  /
  sum(avg_over_time(node_memory_MemTotal_bytes{job=~"calculate_service_seed|calculate_service_clone"}[3m]))
))
```

#### Menghitung Rata-rata Penggunaan Memory dan CPU usage

```
avg(
  100 - (rate(node_cpu_seconds_total{job=~"calculate_service_seed|calculate_service_clone", mode="idle"}[2m]) * 100)
)
+
avg(
  100 * (1 - (
    sum(avg_over_time(node_memory_MemAvailable_bytes{job=~"calculate_service_seed|calculate_service_clone"}[3m]))
    /
    sum(avg_over_time(node_memory_MemTotal_bytes{job=~"calculate_service_seed|calculate_service_clone"}[3m]))
  ))
)
/ 2
```

### Mengatur Contact Point

Contact Point adalah mekanisme untuk mengirimkan notifikasi ketika suatu kondisi alert rule terpenuhi. Contact Point bertindak sebagai penghubung antara sistem monitoring di Grafana dengan platform komunikasi yang digunakan untuk menerima notifikasi, seperti email, Slack, Microsoft Teams, PagerDuty, atau webhook.

Pada bagian ini kita akan membuat contact point berupa webhook. Webhook ini adalah endpoint dari webhook handler yang dibuat pada bagian sebelumnya.

Webhook handler inilah yang kemudian akan menginisiasi proses scaling.

Untuk membuat contact point baru, dibawah bagian alerting klik "Contact points" lalu klik "Create contact point"

:image-display{src="/blog-img/idcloudhost-autoscaling/contact-point-menu.png"}

Berikut konfigurasi contact point untuk setiap event scaling

#### 1. Contact Point Untuk Event Scale Down

:image-display{src="/blog-img/idcloudhost-autoscaling/scaledown-contactpoint.png"}

#### 2. Contact Point Untuk Event Scale Up

:image-display{src="/blog-img/idcloudhost-autoscaling/scaleup-contactpoint.png"}

### Mengatur Alert Rule Untuk Event Scale Up

Pada menu sidebar, pada bgaian alert rule, klik "Alert rules". Lalu klik "New alert rule".

Lalu masukan pengaturan berikut:

#### 1. Scale Up: Enter alert rule name

Masukan nama dari alert rule

#### 2. Scale Up: Query and Alert Condition

Ubah mode pembuatan query dari "Builder" ke "Code" lalu masukan kode query. Sesuaikan query dengan karakteristik aplikasi.

Pada artikel ini alert akan di-trigger jika CPU Usage melebihi 70%. Sesuaikan batas ini sesuai dengan aplikasi yang dijalankan. Pada tingkat penggunaan berapa persen sistem mulai mengembalikan response error?

:image-display{src="/blog-img/idcloudhost-autoscaling/query-and-threshold.png"}

#### 3. Scale Up: Set Evaluation Behaviour

:image-display{src="/blog-img/idcloudhost-autoscaling/set-eval-behaviour.png"}

#### 4. Scale Up: Configure Labels and Notification

Pilih contact point scale up yang sudah dibuat pada bagian sebelumnya

:image-display{src="/blog-img/idcloudhost-autoscaling/label-and-notification.png"}

#### 5. Scale Up: Configure Notification Message

Bagian ini opsional

### Mengatur Alert Rule Untuk Event Scale Down

#### 1. Scale Down: Enter alert rule name

Masukan nama dari alert rule

#### 2. Scale Down: Query and Alert Condition

Sesuaikan query dengan karakteristik aplikasi.

:image-display{src="/blog-img/idcloudhost-autoscaling/query-and-threshold-scaledown.png"}

#### 3. Scale Down: Set Evaluation Behaviour

:image-display{src="/blog-img/idcloudhost-autoscaling/set-eval-behaviour-scaledown.png"}
:image-display{src="/blog-img/idcloudhost-autoscaling/mute-timing-scaledown.png"}

#### 4. Scale Down: Configure Labels and Notification

Pilih contact point scale down

#### 5. Scale Down: Configure Notification Message

Bagian ini opsional

## Load Testing Untuk Memastikan Fungsionalitas Arsitektur Secara Keseluruhan.

Load testing adalah bentuk tes dengan mengirimkan request ke aplikasi dengan beban bervariasi untuk melihat reaksi sistem pada beragam volume request.

Pada bagian ini akan dijelaskan pembuatan load testing untuk mentrigger event scale up dan scale down

Load testing akan menggunakan Grafana K6 yang dijalankan di komputer lokal, berikut adalah panduan membuat kode load testing menggunakan K6

### Merancang Kode Testing

Untuk melakukan testing fitur scale down dan scale up kita harus bisa mengatur secara presisi berapa banyak request yang dikimkan ke server. Artinya kita harus bisa mengendalikan Request per second (RPS), ini diperlukan agar kita bisa mengontrol CPU Load secara presisi.

Selain itu kita harus bisa mengatur testing yang ramping artinya testing yang seiring waktu meningkat untuk mensimulasikan trafic pada sehingga mentrigger scale up lalu menurunkan traffic untuk mensimulasikan trafic yang sepi sehingga mentrigger scale down.

Untuk memenuhi kebutuhan ini kita akan menggunakan executor dari K6 yaitu `rampign-arrival-rate` . Executor ini memungkinkan kita untuk mengatur RPS secara presisi dan menaikan atau menurunkan RPS seiring waktu.

Kita juga akan menggunakan K6 reporter berupa HTML Report untuk menvisualisasikan hasil dari load testing. Saat load testing berakhir, akan muncul file `summary.html` yang berisi hasil load testing. Berikut panduan menggunakan K6 untuk load testing.

1. Instalasi K6

   Pergi ke halaman dokumentasi ini dan install K6 sesuai dengan OS.

   https://grafana.com/docs/k6/latest/set-up/install-k6/

2. Buat sebuah file javascript dan isi dengan kode berikut:

   ```js [loadTest.js]
   import http from "k6/http";
   import { check } from "k6";
   import { htmlReport } from "https://raw.githubusercontent.com/benc-uk/k6-reporter/main/dist/bundle.js";

   // Akan dijalankan skenario 'endpoint_test'
   // Menggunakan ramping arrival state executor
   export const options = {
     discardResponseBodies: true,
     scenarios: {
       endpoint_test: {
         executor: "ramping-arrival-rate",

         // Volume request akan dimulai pada 1 request/detik
         startRate: 1,
         timeUnit: "1s",

         // Virtual yagn digunakan berjumlah 25
         preAllocatedVUs: 25,

         // Total durasi test 20 menit
         stages: [
           // 1 menit pertama 1 RPS
           { target: 1, duration: "1m" },

           // 1 menit kemudian volume naik perlahan ke 6 RPS
           { target: 6, duration: "1m" },

           // 13 menit kemudian tetap pada 6 RPS
           { target: 6, duration: "13m" },

           // 2 menit kemudian turun perlahan ke 1 RPS
           { target: 1, duration: "2m" },

           // 3 menit kemudian tetap pada 1 RPS
           { target: 1, duration: "3m" },
         ],
       },
     },
   };

   // Function untuk mengirim request
   export default function () {
     // GANTI IP ADDRESS
     const url = "http://103.171.84.152/calculate";

     const randomNumber = Math.floor(Math.random() * 9999) + 1;

     const body = JSON.stringify({ number: randomNumber });

     const headers = { "Content-Type": "application/json" };

     const res = http.post(url, body, { headers: headers });

     check(res, {
       "status is 200": (r) => r.status === 200,
     });
   }

   // Setup HTML Report
   export function handleSummary(data) {
     return {
       "summary.html": htmlReport(data),
     };
   }
   ```

3. Menjalankan Testing

Buka terminal dan jalankan command berikut:

```jsx
k6 run loadTest.js
```

### Analisis Hasil Testing

Berikut adalah grafik yang menunjukan persentasi penggunaan CPU pada Service seed
:image-display{src="/blog-img/idcloudhost-autoscaling/graph-load-cpu.png"}

Dan berikut adalah grafik hasil dari load testing. Untuk mendapatkan laporan grafik seperti ini load testing harus dijalankan melalui Grafana Cloud.
:image-display{src="/blog-img/idcloudhost-autoscaling/hasil-load-testing.png"}

Berikut adalah perbandingan dari kedua grafik tersebut.
:image-display{src="/blog-img/idcloudhost-autoscaling/perbandingan-graph-testing.png"}

1. Stage 1 dimulai dengan 1 RPS
2. Stage 1 berakhir dan stage 2 dimulai. RPS naik perlahan dari 1 ke 6. Sistem mulai mengalami load yang tinggi
3. Stage 2 berakhir dan stage 3 dimulai. Volume request stabil di 6 RPS. Sistem mengalami lebih dari 70% CPU Load. Scale up diinisiasi.
4. Masih dalam stage 3. Proses scaling selesai dan load dari Service seed terbagi dua dengan server service baru
5. Stage 3 berakhir dan stage 4 dimulai. Volume request turunperlahan ke 1 RPS. Di tengah-tengah stage ini sistem mengalami CPU load dibawah 30%. Scale down dimulai.
6. Stage 4 berakhir dan stage 5 dimulai. Volume request tetap berada di 1 RPS. Scale down selesai sehingga beban kerja server service seed meningkat sedikit. Terdapat juga lonjakan pada Failure rate, ini dikarenakan ketika server clone dihapus, masih ada request yang belum diselesaikan oleh server tersebut.

::dev-todo
Buat bagian kesimpulan dan konsiderasi

- keamanan - arahkan ke artikel yang membahas security
  ::
